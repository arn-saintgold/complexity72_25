{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    ")  # Adjust as needed\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"scripts\"))\n",
    ")  # Adjust as needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scripts.my_text_cleaning import clean_dataframe\n",
    "from scripts.parallel_topic_model import deduplicate_text_and_embeddings\n",
    "from bertopic import BERTopic\n",
    "from transformers import pipeline\n",
    "from bertopic.representation import TextGeneration\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ad369",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dataset = \"cop26_tweets_en\"\n",
    "chosen_dataset = \"covid_tweets_en\"\n",
    "chosen_dataset = \"ukraine_tweets_en\"\n",
    "ur_df = pd.read_parquet(\"./../../data/raw/\" + chosen_dataset + \".parquet\")\n",
    "doc_info = pd.read_csv(\n",
    "    \"./../../data/processed/document_info_\" + chosen_dataset + \".csv\"\n",
    ")[[\"Document\", \"Topic\", \"Representative_document\", \"Name\"]]\n",
    "topic_info = pd.read_csv(\n",
    "    \"./../../data/processed/topic_info_\" + chosen_dataset + \".csv\"\n",
    ")\n",
    "if \"Unnamed: 0\" in topic_info.columns:\n",
    "    topic_info.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "doc_info.Topic = doc_info.Topic.astype(int)\n",
    "embeddings = np.load(\"./../../data/processed/\" + chosen_dataset + \".parquet.npy\")\n",
    "topic_model = BERTopic.load(\"./../../models/with_hashtags/cop26_tweets_en.parquet.topic_model\")\n",
    "cln_df = clean_dataframe(\n",
    "    ur_df, \n",
    "    'text',\n",
    "    phrases_to_remove=[\"&gt;\", \"&lt;\", \"&amp;\", \"RT : \"],\n",
    "    remove_empty=False,\n",
    "    remove_urls=True,\n",
    "    normalize_hashtags=True,\n",
    "    normalize_mentions=True,\n",
    "    user_placeholder=\"user\",\n",
    "    strip_punctuation=False,\n",
    "    lowercase=False,\n",
    "    )\n",
    "unique_docs, unique_embeddings = deduplicate_text_and_embeddings(cln_df, embeddings, 'Cleantext')\n",
    "topic_model = BERTopic.load(\n",
    "    \"./../../models/\" + chosen_dataset + \".parquet.topic_model\",\n",
    "    embedding_model=\"all-mpnet-base-v2\",\n",
    ")\n",
    "print(f\"{len(embeddings)=}\")\n",
    "print(f\"{len(cln_df)=}\")\n",
    "print(f\"{len(unique_docs)=}\")\n",
    "print(f\"{len(doc_info)=}\")\n",
    "print(cln_df.columns)\n",
    "print(len(unique_docs), len(unique_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad181f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, unique_embeddings = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c474b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English + Spanish stopwords\n",
    "stopwords_en = stopwords.words(\"english\")\n",
    "stopwords_es = stopwords.words(\"spanish\")\n",
    "data_specific_stopwords = []\n",
    "if chosen_dataset == \"covid_tweets_en\":\n",
    "    data_specific_stopwords = [\n",
    "        \"covid\",\n",
    "        \"covid19\",\n",
    "        \"coronavirus\",\n",
    "        #\"pandemic\",\n",
    "        #\"virus\",\n",
    "        #\"people\",\n",
    "        #\"get\",\n",
    "        #\"like\",\n",
    "        #\"one\",\n",
    "        #\"new\",\n",
    "        #\"cases\",\n",
    "        #\"health\",\n",
    "        #\"vaccine\",\n",
    "        #\"vaccines\",\n",
    "        #\"vaccinated\",\n",
    "        #\"deaths\",\n",
    "        #\"time\",\n",
    "        #\"year\",\n",
    "        #\"day\",\n",
    "        #\"years\",\n",
    "    ]\n",
    "elif chosen_dataset == \"ukraine_tweets_en\":\n",
    "    data_specific_stopwords = [\n",
    "        \"ukraine\",\n",
    "        \"russia\",\n",
    "        #\"war\",\n",
    "        #\"russian\",\n",
    "        #\"people\",\n",
    "        #\"like\",\n",
    "        #\"one\",\n",
    "        #\"get\",\n",
    "        #\"just\",\n",
    "        #\"know\",\n",
    "        #\"time\",\n",
    "        #\"day\",\n",
    "        #\"year\",\n",
    "        #\"years\",\n",
    "        #\"donbas\",\n",
    "        #\"ukrainian\",\n",
    "        #\"military\",\n",
    "        #\"ukrainians\",\n",
    "        #\"today\",\n",
    "    ]\n",
    "elif chosen_dataset == \"cop26_tweets_en\":\n",
    "    data_specific_stopwords = [\n",
    "        \"cop26\",\n",
    "        #\"climate\",\n",
    "        #\"people\",\n",
    "        #\"like\",\n",
    "        #\"one\",\n",
    "        #\"get\",\n",
    "        #\"just\",\n",
    "        #\"know\",\n",
    "        #\"time\",\n",
    "        #\"day\",\n",
    "        #\"year\",\n",
    "        #\"years\",\n",
    "        #\"action\",\n",
    "        #\"change\",\n",
    "        #\"global\",\n",
    "        #\"world\",\n",
    "        #\"new\",\n",
    "        #\"need\",\n",
    "    ]\n",
    "custom_stopwords = set(\n",
    "    stopwords_en + stopwords_es + [\"http\", \"https\", \"amp\", \"www\", \"com\"] + [\"user\", 'rt'] + data_specific_stopwords\n",
    ")\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean = doc_info.Document.tolist()\n",
    "topics = (doc_info.Topic.tolist())\n",
    "vectorizer_model = CountVectorizer(stop_words=list(custom_stopwords), ngram_range=(1, 2))\n",
    "\n",
    "representation_models = {\n",
    "    \"MMR\": MaximalMarginalRelevance(diversity=0.7)\n",
    "}\n",
    "\n",
    "topic_model.update_topics(\n",
    "    docs=docs_clean, topics=topics, vectorizer_model=vectorizer_model, representation_model=representation_models\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f454e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topic_info = topic_model.get_topic_info()\n",
    "new_topic_info['New_Name'] = new_topic_info.apply(\n",
    "    lambda row: str(row['Topic']) + \"_\" + \"_\".join(row['MMR']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "new_topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfada5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_topic_info = topic_info.merge(\n",
    "    new_topic_info[[\"Topic\", 'MMR', \"New_Name\"]], on=\"Topic\", suffixes=(\"_old\", \"_new\")\n",
    ")\n",
    "merged_topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad834b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_topic_info.to_csv(\n",
    "    \"./../../data/processed/topic_info_\" + chosen_dataset + \"_with_MMR.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452dadc7",
   "metadata": {},
   "source": [
    "### Code for label generation with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I have a topic described by the following keywords: [KEYWORDS]. Based on the previous keywords, what is this topic about?\"\n",
    "\n",
    "# Create your representation model\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "flan_model = TextGeneration(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR Model\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Text generation\n",
    "representation_model = {\"MMR\": mmr_model, \"Flan\": flan_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68de65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "topic_model.update_topics(\n",
    "    unique_docs,\n",
    "    topics=doc_info.Topic.to_list(),\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9af05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in topic_model.get_topic_info()[\"Zephyr\"]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933808b5",
   "metadata": {},
   "source": [
    "## Generation with Zephyr-7B-Î±\n",
    "Failed because of uncaught error in transformers.pipeline:\n",
    "Maybe because of ctransformer model.\n",
    "The huggingface model is too big.\n",
    "```python \n",
    "AttributeError: 'TextGenerationPipeline' object has no attribute 'assistant_model'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "\n",
    "generator = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "print(generator(\"Once upon a time\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from bertopic.representation import TextGeneration\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/zephyr-7B-alpha-GGUF\",\n",
    "    model_file=\"zephyr-7b-alpha.Q3_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    gpu_layers=50,\n",
    "    hf=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e23e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ctransformers[cuda]\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers\n",
    "\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "import transformers\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/zephyr-7B-alpha-GGUF\",\n",
    "    model_file=\"zephyr-7b-alpha.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    gpu_layers=50,\n",
    "    hf=True,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "\n",
    "# Pipeline\n",
    "generator = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f87bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR Model\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Text generation with Zephyr\n",
    "zephyr = TextGeneration(generator, prompt=prompt)\n",
    "representation_model = {\"MMR\": mmr_model, \"Zephyr\": zephyr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf02bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(\n",
    "    unique_docs,\n",
    "    topics=doc_info.Topic.to_list(),\n",
    "    representation_model=representation_model,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
