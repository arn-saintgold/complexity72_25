{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afd8e7e",
   "metadata": {},
   "source": [
    "# Topic Interpretability\n",
    "\n",
    "Compute Diversity, Coherence, or other measures of topic interpretability.\n",
    "This notebook uses octis on Python3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b9cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ukraine_tweets_en\n",
      "N of documents: 787872\n",
      "N of topics: 138\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))  # Adjust as needed\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..','scripts')))  # Adjust as needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scripts.my_text_cleaning import clean_dataframe\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "import ast\n",
    "\n",
    "chosen_dataset = 'covid_tweets_en'\n",
    "chosen_dataset = 'cop26_tweets_en'\n",
    "chosen_dataset = 'ukraine_tweets_en'\n",
    "\n",
    "#ur_df = pd.read_parquet('./../../data/raw/'+chosen_dataset+'.parquet')\n",
    "doc_info = pd.read_csv('./../../data/processed/document_info_'+chosen_dataset+'.csv')[['Document', 'Topic']]\n",
    "doc_info.Topic = doc_info.Topic.astype(int)\n",
    "topic_info = pd.read_csv('./../../data/processed/topic_info_'+chosen_dataset+'_with_MMR.csv')[['Topic', 'Representation', 'MMR']]\n",
    "topic_info.Representation = topic_info.Representation.apply(ast.literal_eval)\n",
    "embeddings = np.load('./../../data/processed/'+chosen_dataset+'.parquet.npy')\n",
    "#unique_docs, unique_embeddings = clean_dataframe(ur_df, embeddings, 'text')\n",
    "print(chosen_dataset)\n",
    "print(f\"N of documents: {len(doc_info)}\")\n",
    "print(f\"N of topics: {len(topic_info)-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc74875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import spacy\n",
    "import pickle\n",
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import os\n",
    "\n",
    "# =====================================\n",
    "# Setup\n",
    "# =====================================\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy.require_gpu()\n",
    "print(\"GPU enabled:\", spacy.prefer_gpu())\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# =====================================\n",
    "# Cleaning functions\n",
    "# =====================================\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" \n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"\\U00002700-\\U000027BF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U00002600-\\U000026FF\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# =====================================\n",
    "# Document cleaning with caching\n",
    "# =====================================\n",
    "def clean_documents_cached(doc_df: pd.DataFrame, text_col: str, cache_path: str, batch_size=3000):\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading preprocessed documents from {cache_path} ...\")\n",
    "        with gzip.open(cache_path, 'rb') as f:\n",
    "            doc_texts = pickle.load(f)\n",
    "        return doc_texts\n",
    "\n",
    "    print(\"Cleaning documents...\")\n",
    "    cleaned_texts = [clean_text(str(doc)) for doc in doc_df[text_col]]\n",
    "    tokenized_texts = [doc.split() for doc in cleaned_texts]\n",
    "\n",
    "    print(\"Lemmatizing documents...\")\n",
    "    lemmatized_texts = []\n",
    "    for doc_batch in tqdm(nlp.pipe([\" \".join(doc) for doc in tokenized_texts],\n",
    "                                   batch_size=batch_size), total=len(tokenized_texts), desc=\"Processing docs\"):\n",
    "        lemmatized_texts.append([token.lemma_ for token in doc_batch if token.lemma_ != ''])\n",
    "\n",
    "    print(f\"Saving preprocessed documents to {cache_path} ...\")\n",
    "    with gzip.open(cache_path, 'wb') as f:\n",
    "        pickle.dump(lemmatized_texts, f)\n",
    "\n",
    "    return lemmatized_texts\n",
    "\n",
    "# =====================================\n",
    "# Topic cleaning\n",
    "# =====================================\n",
    "def clean_topics(topic_df: pd.DataFrame, represetation_col:str, topic_col: str = 'Topic', exclude_topics=None, min_words=3):\n",
    "    exclude_topics = exclude_topics or []\n",
    "    filtered = topic_df.loc[~topic_df[topic_col].isin(exclude_topics), :].copy()\n",
    "    # Ensure Representation column contains lists\n",
    "    filtered[topic_col] = filtered[topic_col].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    cleaned_topics = []\n",
    "    for topic in tqdm(filtered[represetation_col], total=len(filtered), desc=\"Processing topics\"):\n",
    "        topic_clean = [clean_text(str(t)) for t in topic if t]\n",
    "        if not topic_clean:\n",
    "            continue\n",
    "        doc = list(nlp.pipe([\" \".join(topic_clean)], batch_size=1))[0]\n",
    "        lemmatized_topic = [token.lemma_ for token in doc if token.lemma_ != '']\n",
    "        if len(lemmatized_topic) >= min_words:\n",
    "            cleaned_topics.append(lemmatized_topic)\n",
    "    return cleaned_topics\n",
    "\n",
    "# =====================================\n",
    "# Compute coherence and diversity\n",
    "# =====================================\n",
    "def compute_coherence_diversity(doc_texts, topic_list, coherence_measure='c_v', topics = None, exclude_topics=None):\n",
    "    # Automatically set topk to the minimum topic length\n",
    "    print(f\"{len(doc_texts)} documents, {len(topic_list)} topics\")\n",
    "    if exclude_topics is not None and topics is not None:\n",
    "        doc_texts = [doc for i, doc in enumerate(doc_texts) if topics[i] not in exclude_topics]\n",
    "    print(f\"{len(doc_texts)} documents, {len(topic_list)} topics\")\n",
    "    min_topic_len = min(len(t) for t in topic_list)\n",
    "    topk = min(10, min_topic_len)\n",
    "    print(f\"Using topk={topk} for coherence computation (min topic length={min_topic_len})\")\n",
    "\n",
    "    coherence = Coherence(texts=doc_texts, measure=coherence_measure, topk=topk)\n",
    "    coherence_dict = {\"topics\": topic_list}\n",
    "    coherence_score = coherence.score(coherence_dict)\n",
    "\n",
    "    diversity = TopicDiversity(topk=topk)\n",
    "    diversity_dict = {\"topics\": topic_list}\n",
    "    diversity_score = diversity.score(diversity_dict)\n",
    "\n",
    "    return coherence_score, diversity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b427161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning documents...\n",
      "Lemmatizing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b509ac49f07e4df7b8440df4436558c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing docs:   0%|          | 0/787872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed documents to ./../../data/processed/ukraine_tweets_enclean_lemmatized_docs.pkl.gz ...\n"
     ]
    }
   ],
   "source": [
    "# Clean and cache documents (once per dataset)\n",
    "doc_cache_path = './../../data/processed/'+chosen_dataset+'clean_lemmatized_docs.pkl.gz'\n",
    "doc_texts = clean_documents_cached(doc_info, text_col='Document', cache_path=doc_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8faf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06f549a742244bf86bc8d6eec669704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing topics:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787872 documents, 138 topics\n",
      "404922 documents, 138 topics\n",
      "Using topk=10 for coherence computation (min topic length=10)\n",
      "Coherence: 0.6096611121621709\n",
      "Diversity: 0.5007246376811594\n"
     ]
    }
   ],
   "source": [
    "# Clean topics, excluding topic -1 if needed\n",
    "exclude_topics = None\n",
    "exclude_topics = [-1]  # you can change this dynamically\n",
    "representation = 'MMR'\n",
    "representation = 'Representation'\n",
    "topics_cleaned = clean_topics(topic_info, represetation_col=representation, exclude_topics=exclude_topics, min_words=1)\n",
    "# Compute coherence & diversity on saved documents and filtered topics\n",
    "coherence_score, diversity_score = compute_coherence_diversity(doc_texts, topics_cleaned, coherence_measure='c_v', exclude_topics=exclude_topics, topics=doc_info['Topic'].tolist())\n",
    "print(\"Coherence:\", coherence_score)\n",
    "print(\"Diversity:\", diversity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95460537",
   "metadata": {},
   "source": [
    "| Dataset | N Docs | Min Clust Size |N Topics | Noise % | RV | C_V | Diversity | WED |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | \n",
    "| Cop26 | 105,383 | 400 | 27 |  22.15% | 39.67% | 49.12% | 49.64% | 80.88% | \n",
    "| Covid | 545,032 | 250 | 163 | 41.74% | 34.75% | 55.13% | 43.78% | 78.43% |\n",
    "| Ukraine | 787,872 | 500 | 137 | 48.61% | 32.35% | 61.03% | 49.71% | 74.04% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97eddd2",
   "metadata": {},
   "source": [
    "## Examine Topics\n",
    "Manual examination of some of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# INPUTS: adjust to your data\n",
    "# -----------------------------\n",
    "# doc_info: DataFrame with at least columns ['Document', 'Topic', 'Representative_document']\n",
    "# topic_info: DataFrame with at least columns ['Representation'] (list of top words per topic)\n",
    "# num_topics_sample: number of topics to inspect\n",
    "# num_docs_per_topic: number of documents to show per topic\n",
    "\n",
    "num_topics_sample = 10\n",
    "num_docs_per_topic = 3\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: sample topics\n",
    "# -----------------------------\n",
    "def sample_topics(topic_info, num_samples):\n",
    "    total_topics = len(topic_info)\n",
    "    if num_samples >= total_topics:\n",
    "        return list(range(total_topics))\n",
    "    \n",
    "    # sample proportionally to topic size (number of assigned documents)\n",
    "    topic_sizes = doc_info['Topic'].value_counts().reindex(range(total_topics), fill_value=0).tolist()\n",
    "    probabilities = [s / sum(topic_sizes) for s in topic_sizes]\n",
    "    sampled_idx = random.choices(range(total_topics), k=num_samples, weights=probabilities)\n",
    "    return sampled_idx\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: show representative documents\n",
    "# -----------------------------\n",
    "def show_topic(topic_idx):\n",
    "    print(f\"\\n=== Topic {topic_idx} ===\")\n",
    "    top_words = topic_info.loc[topic_idx, 'Representation']\n",
    "    print(\"Top words:\", top_words)\n",
    "    \n",
    "    # get documents assigned to this topic\n",
    "    docs = doc_info.loc[doc_info['Topic'] == topic_idx, 'Representative_document']\n",
    "    \n",
    "    if docs.empty:\n",
    "        print(\"No documents assigned to this topic.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Showing {min(num_docs_per_topic, len(docs))} representative documents:\")\n",
    "    for i, doc in enumerate(docs.head(num_docs_per_topic), start=1):\n",
    "        print(f\"Doc {i}: {doc[:300]}{'...' if len(doc) > 300 else ''}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run inspection\n",
    "# -----------------------------\n",
    "sampled_topics = sample_topics(topic_info, num_topics_sample)\n",
    "\n",
    "for idx in sampled_topics:\n",
    "    show_topic(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad96bc1",
   "metadata": {},
   "source": [
    "## Word-Embedding-based Diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a9a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding-based Diversity (WED): 0.7404\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load topics from BERTopic\n",
    "# -------------------------\n",
    "\n",
    "topic_words = topic_info['Representation'].tolist()\n",
    "\n",
    "# -------------------------\n",
    "# 2. Load pre-trained embeddings\n",
    "# -------------------------\n",
    "# Example: GoogleNews word2vec (download first from GoogleNews-vectors-negative300.bin.gz)\n",
    "path_to_embeddings = \"../../models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "w2v = KeyedVectors.load_word2vec_format(path_to_embeddings, binary=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Compute WED\n",
    "# -------------------------\n",
    "def compute_wed(topics, model):\n",
    "    all_words = set(w for topic in topics for w in topic if w in model)\n",
    "    vectors = {w: model[w] for w in all_words}\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for topic in topics:\n",
    "        words_in_vocab = [w for w in topic if w in vectors]\n",
    "        for i in range(len(words_in_vocab)):\n",
    "            for j in range(i + 1, len(words_in_vocab)):\n",
    "                v1, v2 = vectors[words_in_vocab[i]], vectors[words_in_vocab[j]]\n",
    "                dist = cosine_distances([v1], [v2])[0][0]\n",
    "                pairwise_distances.append(dist)\n",
    "\n",
    "    return np.mean(pairwise_distances) if pairwise_distances else 0.0\n",
    "\n",
    "wed_score = compute_wed(topic_words, w2v)\n",
    "print(f\"Word Embedding-based Diversity (WED): {wed_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fea7ba",
   "metadata": {},
   "source": [
    "### Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef1540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "\n",
    "# =====================================\n",
    "# Setup\n",
    "# =====================================\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # small model\n",
    "spacy.require_gpu()  # enable GPU\n",
    "print(\"GPU enabled:\", spacy.prefer_gpu())\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(set(stopwords.words('spanish'))).union(['rt', 'via', '…'])\n",
    "\n",
    "# =====================================\n",
    "# Cleaning functions\n",
    "# =====================================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove emojis, mentions, links, hashtags, extra whitespace.\"\"\"\n",
    "    text = str(text)\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" \n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"\\U00002700-\\U000027BF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U00002600-\\U000026FF\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_tokens(tokens):\n",
    "    \"\"\"Remove stopwords and lemmatize using spaCy (GPU).\"\"\"\n",
    "    tokens_nostop = [w for w in tokens if w.lower() not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens_nostop))\n",
    "    return [token.lemma_ for token in doc if token.lemma_ != '']\n",
    "\n",
    "# =====================================\n",
    "# Document cleaning with GPU + tqdm\n",
    "# =====================================\n",
    "def clean_documents(series: pd.Series, batch_size=3000) -> list:\n",
    "    \"\"\"Clean documents using nlp.pipe with progress bar.\"\"\"\n",
    "    print(\"Cleaning documents...\")\n",
    "    cleaned_texts = [clean_text(str(doc)) for doc in series]\n",
    "    print(\"Tokenizing documents...\")\n",
    "    tokenized_texts = [doc.split() for doc in cleaned_texts]\n",
    "    print(\"Lemmatizing documents...\")\n",
    "    lemmatized_texts = []\n",
    "    # tqdm wrapper for nlp.pipe\n",
    "    for doc_batch in tqdm(nlp.pipe([\" \".join(doc) for doc in tokenized_texts], batch_size=batch_size), \n",
    "                          total=len(tokenized_texts), desc=\"Processing docs\"):\n",
    "        lemmatized_texts.append([token.lemma_ for token in doc_batch if token.lemma_ != ''])\n",
    "    print(\"Document cleaning completed.\")\n",
    "    return lemmatized_texts\n",
    "\n",
    "# =====================================\n",
    "# Topic cleaning with GPU + tqdm\n",
    "# =====================================\n",
    "def clean_topics(series: pd.Series, min_words=3, batch_size=500) -> list:\n",
    "    \"\"\"Clean topic words, lemmatize, remove short topics.\"\"\"\n",
    "    cleaned_topics = []\n",
    "    print(\"Cleaning topics...\")\n",
    "    # Ensure series contains lists\n",
    "    series = series.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Use tqdm with manual iteration for topics\n",
    "    for topic in tqdm(series, total=len(series), desc=\"Processing topics\"):\n",
    "        topic_clean = [token for token in topic if token]\n",
    "        topic_clean = [clean_text(str(t)) for t in topic_clean]\n",
    "        if not topic_clean:\n",
    "            continue\n",
    "\n",
    "        # Lemmatize using nlp.pipe with batch_size=1\n",
    "        doc = list(nlp.pipe([\" \".join(topic_clean)], batch_size=1))[0]\n",
    "        lemmatized_topic = [token.lemma_ for token in doc if token.lemma_ != '']\n",
    "\n",
    "        if len(lemmatized_topic) >= min_words:\n",
    "            cleaned_topics.append(lemmatized_topic)\n",
    "\n",
    "    return cleaned_topics\n",
    "\n",
    "# =====================================\n",
    "# Compute coherence with auto topk\n",
    "# =====================================\n",
    "def compute_coherence(doc_series, topic_series):\n",
    "    # Clean data\n",
    "    doc_texts = clean_documents(doc_series)\n",
    "    topics_cleaned = clean_topics(topic_series, min_words=1)  # min_words=1 to keep all topics\n",
    "\n",
    "    # Automatically set topk to the minimum topic length\n",
    "    min_topic_len = min(len(t) for t in topics_cleaned)\n",
    "    topk = min(10, min_topic_len)  # default 10 or shorter if needed\n",
    "\n",
    "    print(f\"Using topk={topk} for coherence computation (min topic length={min_topic_len})\")\n",
    "\n",
    "    coherence = Coherence(texts=doc_texts, measure='c_v', topk=topk)\n",
    "    coherence_dict = {\"topics\": topics_cleaned}\n",
    "    coherence_score = coherence.score(coherence_dict)\n",
    "\n",
    "    diversity = TopicDiversity(topk=topk)\n",
    "    diversity_dict = {\"topics\": topics_cleaned}\n",
    "    diversity_score = diversity.score(diversity_dict)\n",
    "    return coherence_score, diversity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dea7486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning documents...\n",
      "Tokenizing documents...\n",
      "Lemmatizing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ea132a83a9400095b75fa41b54070d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing docs:   0%|          | 0/105383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m topic_coherence_score, topic_diversity_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRepresentation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(topic_coherence_score, topic_diversity_score)\n",
      "Cell \u001b[0;32mIn[10], line 96\u001b[0m, in \u001b[0;36mcompute_coherence\u001b[0;34m(doc_series, topic_series)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_coherence\u001b[39m(doc_series, topic_series):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Clean data\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     doc_texts \u001b[38;5;241m=\u001b[39m \u001b[43mclean_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_series\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     topics_cleaned \u001b[38;5;241m=\u001b[39m clean_topics(topic_series, min_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# min_words=1 to keep all topics\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Automatically set topk to the minimum topic length\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 59\u001b[0m, in \u001b[0;36mclean_documents\u001b[0;34m(series, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m lemmatized_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# tqdm wrapper for nlp.pipe\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(nlp\u001b[38;5;241m.\u001b[39mpipe([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tokenized_texts], batch_size\u001b[38;5;241m=\u001b[39mbatch_size), \n\u001b[1;32m     60\u001b[0m                       total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tokenized_texts), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing docs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     61\u001b[0m     lemmatized_texts\u001b[38;5;241m.\u001b[39mappend([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc_batch \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument cleaning completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/language.py:1622\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1621\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1623\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/util.py:1714\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1705\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1706\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1712\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1714\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:245\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/util.py:1661\u001b[0m, in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1660\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[0;32m-> 1661\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/util.py:1714\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1705\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1706\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1712\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1714\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/pipeline/pipe.pyx:48\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/util.py:1714\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1705\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1706\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1712\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1714\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/pipeline/pipe.pyx:48\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/util.py:1714\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1705\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1706\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1712\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1714\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:249\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/prog-projects/complexity72_25/.octis/lib/python3.10/site-packages/spacy/util.py:1664\u001b[0m, in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "topic_coherence_score, topic_diversity_score = compute_coherence(doc_info['Document'], topic_info['Representation'])\n",
    "print(topic_coherence_score, topic_diversity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611b75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning documents...\n",
      "Tokenizing documents...\n",
      "Lemmatizing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c51df6686345ddaf645e0d130e6ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing docs:   0%|          | 0/372470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_coherence_score, topic_diversity_score = compute_coherence(doc_info.query(\"Topic != -1\")['Document'], topic_info.query(\"Topic != -1\")['Representation'])\n",
    "print(topic_coherence_score, topic_diversity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464c26d",
   "metadata": {},
   "source": [
    "METRICS NOISELESS\n",
    "\n",
    "Cop26: \n",
    "Coherence = 0.5711331303192272\n",
    "Diversity = 0.53125\n",
    "\n",
    "Covid\n",
    "Coherence = 0.5350817136076244\n",
    "Diversity = 0.4392\n",
    "\n",
    "Ukraine\n",
    "Coherence = 0.6236733698956629\n",
    "Diversity = 0.4772151898734177"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385bc1df",
   "metadata": {},
   "source": [
    "METRICS WITH NOISE\n",
    "\n",
    "Cop26: \n",
    "Coherence = 0.5514339316450143\n",
    "Diversity = 0.5224489795918368\n",
    "\n",
    "Covid\n",
    "Coherence = 0.5435738180981629\n",
    "Diversity = 0.4357142857142857\n",
    "\n",
    "Ukraine\n",
    "Coherence = 0.6041440436052993\n",
    "Diversity = 0.4742138364779874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85676178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".octis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
